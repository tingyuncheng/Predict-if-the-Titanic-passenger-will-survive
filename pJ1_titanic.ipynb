{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pJ1_titanic.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "175ScpYJp92SBM7npLEP1tQFtLQ9LxQUY",
      "authorship_tag": "ABX9TyO2sDorrev8XOUx+sj0C759"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdWb2C8RgHn5"
      },
      "source": [
        "**Subject**: \n",
        "\n",
        "Kaggle - Titanic (https://www.kaggle.com/c/titanic)\n",
        "\n",
        "**Goal:** \n",
        "\n",
        "To predict if the passenger will survive. As my goal is clear, this is supervised learning. I am gonna use the data we know to build two models, Logistics Regression model and Decision Tree, apply the models to the validation dataset, and score the record to make the best guess/prediction.\n",
        "\n",
        "For this project:\n",
        "\n",
        "I am predicting a binary, categorical target(outcome) variable.\n",
        "\n",
        "Each row is a record/case (here means a passenger record).\n",
        "\n",
        "Each column is a variable/feature.\n",
        "\n",
        "**Main packages I use**\n",
        "- pandas\n",
        "- matplotlib.pyplot\n",
        "- numpy\n",
        "- sklearn\n",
        "- yellowbrick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC8GnmhV-LE2"
      },
      "source": [
        "I'm sharing two methods to upload the data.\n",
        "\n",
        "|| First method: Upload files to colab ||"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WijEZXwdjqq"
      },
      "source": [
        "# import package\n",
        "# upload files to colab\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Store dataset in a Pandas Dataframe\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "data = pd.read_csv(io.BytesIO(uploaded['train.csv']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFW91E6Q-XMS"
      },
      "source": [
        "|| Second method: use google drive ||"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2bYjwdU7v0p"
      },
      "source": [
        "# import package\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization\n",
        "# The reason for the force_remount is that I run multiple times.\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/pJ1_Titanic/train.csv\"\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/pJ1_Titanic/train.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukajyPpjAqyN"
      },
      "source": [
        "|| A glance at how the dataset looks like ||\n",
        "- Any missing values?\n",
        "- Any outliers?\n",
        "- How unique the dataset is?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hir13OVhZBg"
      },
      "source": [
        "# Check the data shape and content\n",
        "print(\"The dimension of the table is:\", data.shape)\n",
        "pd.DataFrame(data.head(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71kJyGQ2gdB7"
      },
      "source": [
        "# Check what the variables look like\n",
        "# Numerical variable result\n",
        "\n",
        "print(data.describe())\n",
        "\n",
        "# Categorical variable result - \"unique\"-show # of categories in each variable\n",
        "\n",
        "data.describe(include=['O'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iqWFnjQMP8t"
      },
      "source": [
        "***For Categorical variable result***\n",
        "\n",
        "**Cabin**: Contain way too many missing values, and many unique record, may exclude it.\n",
        "\n",
        "**Embarked**: Just two missing values, will think about what to do later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDMt_traiWeM"
      },
      "source": [
        "|| Pre-processing Data ||\n",
        "\n",
        "**80% of work happens here. There are some questions we can ask ourselves.**\n",
        "\n",
        "1) What is trget variable, what are features?\n",
        "  - What's the distribution?\n",
        "  - How many in different categories?\n",
        "\n",
        "2) Are the numerical variable correlated?\n",
        "\n",
        "3) Correlation between the target variable and numeric features? (e.g. Scatter plot - younger, more likely to survive?)\n",
        "\n",
        "4) Different survival rate in different categories? (e.g. Women more likely to survive?)\n",
        "\n",
        "____________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd2l-0BhjSqH"
      },
      "source": [
        "STEP 1_1) What is target variable, what are features? \n",
        "\n",
        "- What's the distribution?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne4GvTXTjLap"
      },
      "source": [
        "# import visualization package\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# set up the figure size\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (20,10)\n",
        "\n",
        "# make subplots - how many figure do I want? 2x2\n",
        "fig, axes = plt.subplots(nrows = 3, ncols = 2)\n",
        "\n",
        "# Specifiy the features of interest - Define xaxes & yaxes\n",
        "num_features = ['Age', 'SibSp', 'Parch', 'Fare', 'Ticket', 'Cabin']\n",
        "xaxes = num_features\n",
        "yaxes = ['Counts', 'Counts', 'Counts', 'Counts', 'Counts', 'Counts']\n",
        "\n",
        "# draw histograms\n",
        "# loop over the axes\n",
        "# The reason for [idx] is to loop each in sequence, otherwise is a total value\n",
        "axes = axes.ravel()\n",
        "for idx, ax in enumerate(axes):\n",
        "    ax.hist(data[num_features[idx]].dropna(), bins=20)\n",
        "    ax.set_xlabel(xaxes[idx], fontsize=10)\n",
        "    ax.set_ylabel(yaxes[idx], fontsize=10)\n",
        "    ax.tick_params(axis='both', labelsize=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "036gxP-DenFq"
      },
      "source": [
        "**Features analysis**\n",
        "\n",
        "**Age**: From the distribution, it looks ok. From \"data.describe()\" it shows there are 177 missing values. As age is a key element for our predictive model, I'll do something about it later.\n",
        "\n",
        "**Fare**: From the distribution, it is seriously right-skewed. I may need to exclude outliers (if not too many) and/or just normalized this feature. \n",
        "\n",
        "**SibSp** and **Parch** seem like numerical variables, but have categorical variables traits. Not consider them as skewed.\n",
        "\n",
        "**Ticket** and **Cabin**, no use for this model. Exculde them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF7NdhJsHnQL"
      },
      "source": [
        "# Update my features set for later use\n",
        "num_features = ['Age', 'SibSp', 'Parch', 'Fare']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD8Vc-1Ejfh0"
      },
      "source": [
        "STEP 1_2) What is target variable, what are features? \n",
        "\n",
        "- How many in different categories?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfngsIoMkF68"
      },
      "source": [
        "#||Create Bar plot to explore Categorical data||\n",
        "# Each bar plot need to do it seperatly. Can't loop!\n",
        "\n",
        "# set up the figure size\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (20,10)\n",
        "\n",
        "# make subplots\n",
        "fig, axes = plt.subplots(nrows = 2, ncols = 2)\n",
        "\n",
        "# - - make the data read to feed into the visulizer\n",
        "X_Survived = data.replace({'Survived':{1:'yes', 0:'no'}}).groupby('Survived').size().reset_index(name='Counts')['Survived']\n",
        "Y_Survived = data.replace({'Survived':{1:'yes', 0:'no'}}).groupby('Survived').size().reset_index(name='Counts')['Counts']\n",
        "\n",
        "# Make the bar plot -- upper left axes[0,0]\n",
        "axes[0,0].bar(X_Survived, Y_Survived)\n",
        "axes[0,0].set_title('Survived', fontsize=25)\n",
        "axes[0,0].set_ylabel('Counts', fontsize=20)\n",
        "axes[0,0].tick_params(axis='both', labelsize=15)\n",
        "\n",
        "# - - make the data read to feed into the visulizer\n",
        "X_Pclass = data.replace({'Pclass':{1:'1st', 2:'2nd', 3:'3rd'}}).groupby('Pclass').size().reset_index(name='Counts')['Pclass']\n",
        "Y_Pclass = data.replace({'Pclass':{1:'1st', 2:'2nd', 3:'3rd'}}).groupby('Pclass').size().reset_index(name='Counts')['Counts']\n",
        "\n",
        "# Make the bar plot -- upper right axes[0,1]\n",
        "axes[0,1].bar(X_Pclass, Y_Pclass)\n",
        "axes[0,1].set_title('Pclass', fontsize=25)\n",
        "axes[0,1].set_ylabel('Counts', fontsize=20)\n",
        "axes[0,1].tick_params(axis='both', labelsize=15)\n",
        "\n",
        "# - - make the data read to feed into the visulizer\n",
        "X_Sex = data.groupby('Sex').size().reset_index(name='Counts')['Sex']\n",
        "Y_Sex = data.groupby('Sex').size().reset_index(name='Counts')['Counts']\n",
        "\n",
        "# Make the bar plot -- lower left axes[1,0]\n",
        "axes[1,0].bar(X_Sex, Y_Sex)\n",
        "axes[1,0].set_title('Sex', fontsize=25)\n",
        "axes[1,0].set_ylabel('Counts', fontsize=20)\n",
        "axes[1,0].tick_params(axis='both', labelsize=15)\n",
        "\n",
        "# - - make the data read to feed into the visulizer\n",
        "X_Embarked = data.groupby('Embarked').size().reset_index(name='Counts')['Embarked']\n",
        "Y_Embarked = data.groupby('Embarked').size().reset_index(name='Counts')['Counts']\n",
        "\n",
        "# Make the bar plot -- lower right axes[1,1]\n",
        "axes[1,1].bar(X_Embarked, Y_Embarked)\n",
        "axes[1,1].set_title('Embarked', fontsize=25)\n",
        "axes[1,1].set_ylabel('Counts', fontsize=20)\n",
        "axes[1,1].tick_params(axis='both', labelsize=15)\n",
        "\n",
        "# Note:\n",
        "# Can't just groupby Survived and Pclass, because its data is in number format."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC9aAsRAju_c"
      },
      "source": [
        "STEP 2) Are the numerical variable correlated?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzJhMnQikGZx"
      },
      "source": [
        "# Create Pearson Ranking visualization\n",
        "\n",
        "# Set up the figure size\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15,7)\n",
        "\n",
        "# import the package for visualization of the correlation\n",
        "from yellowbrick.features import Rank2D\n",
        "\n",
        "# extract the numpy arrays from the data frame\n",
        "x = data[num_features].to_numpy()\n",
        "\n",
        "# instantiate the 2D visualizer with the Pearson ranking algorithm\n",
        "visualizer = Rank2D(features=num_features, algorithm='pearson')\n",
        "visualizer.fit(x)             # Fit the data to the visualizer\n",
        "visualizer.transform(x)       # Transform the data\n",
        "visualizer.poof()             # Finalize and show the figure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BptsHorGj676"
      },
      "source": [
        "STEP 3) Correlation between the target variable and numeric features? (Scatter plot - younger, more likely to survive?)\n",
        "\n",
        "- compare the distributions of numerical variables between passengers that survived and those that did not survive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdmpuEAAkG3n"
      },
      "source": [
        "# Create ParallelCoordinates\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15,7)\n",
        "plt.rcParams['font.size'] = 50\n",
        "\n",
        "# setup the color for yellowbrick visualizer\n",
        "from yellowbrick.style import set_palette\n",
        "set_palette('reset')\n",
        "\n",
        "# import packages\n",
        "from yellowbrick.features import ParallelCoordinates\n",
        "\n",
        "# specify the features of interest and the classes of the target\n",
        "classes = ['Not-survived', 'Survived']\n",
        "num_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "\n",
        "# copy data to a new DataFrame\n",
        "data_norm = data.copy()\n",
        "\n",
        "# normalize data to 0-1 range\n",
        "# Tips to normalize data by using Pandas:\n",
        "# (1) normalized_df=(df-df.mean())/df.std()\n",
        "# (2) normalized_df=(df-df.min())/(df.max()-df.min())\n",
        "\n",
        "for feature in num_features:\n",
        "    data_norm[feature] = (data[feature] - data[feature].mean())/ (data[feature].max() - data[feature].min())\n",
        "\n",
        "# Extract the Numpy arrays from the DataFrame\n",
        "X = data_norm[num_features].to_numpy()\n",
        "y = data.Survived.to_numpy()\n",
        "\n",
        "# Instantiate the visualizer\n",
        "visualizer = ParallelCoordinates(classes=classes, features=num_features)\n",
        "\n",
        "visualizer.fit(X, y)      # Fit the data to the visualizer\n",
        "visualizer.transform(x)   # Transform the data\n",
        "visualizer.poof()         # Finalize and show the visualizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugeAFeyjj9ex"
      },
      "source": [
        "STEP 4) Different survival rate in different categories? \n",
        "\n",
        "(Correlation between the target variable and categorical features)\n",
        "\n",
        "(Women more likely to survive?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32-ZBTEEkHOM"
      },
      "source": [
        "# Create bar plots\n",
        "# set up the figure size\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (20,10)\n",
        "\n",
        "# make subplots\n",
        "fig, axes = plt.subplots(nrows = 2, ncols = 2)\n",
        "\n",
        "# - - make the data read to feed into the visulizer\n",
        "Sex_survived = data.replace({'Survived':{1: 'Survived', 0: 'Not-survived'}})[data['Survived']==1]['Sex'].value_counts()\n",
        "Sex_not_survived = data.replace({'Survived': {1: 'Survived', 0: 'Not-survived'}})[data['Survived']==0]['Sex'].value_counts()\n",
        "Sex_not_survived = Sex_not_survived.reindex(index = Sex_survived.index) # sex_survived as index at the bottom\n",
        "\n",
        "# Make the bar plot\n",
        "p1 = axes[0, 0].bar(Sex_survived.index, Sex_survived.values)\n",
        "p2 = axes[0, 0].bar(Sex_not_survived.index, Sex_not_survived.values, bottom=Sex_survived.values)\n",
        "axes[0, 0].set_title('Sex', fontsize=25)\n",
        "axes[0, 0].set_ylabel('Counts', fontsize=20)\n",
        "axes[0, 0].tick_params(axis='both', labelsize=15)\n",
        "axes[0, 0].legend((p1[0], p2[0]), ('Survived', 'Not-survived'), fontsize = 15) #display legend categories\n",
        "\n",
        "# - - make the data read to feed into the visulizer\n",
        "Pclass_survived = data.replace({'Survived': {1: 'Survived', 0: 'Not-survived'}}).replace({'Pclass':{1: '1st', 2: '2nd', 3:'3rd'}})[data['Survived']==1]['Pclass'].value_counts()\n",
        "Pclass_not_survived = data.replace({'Survived': {1: 'Survived', 0: 'Not-survived'}}).replace({'Pclass':{1: '1st', 2: '2nd', 3:'3rd'}})[data['Survived']==0]['Pclass'].value_counts()\n",
        "Pclass_not_survived = Pclass_not_survived.reindex(index = Pclass_survived.index) # Pclass_survived as index at the bottom\n",
        "\n",
        "# Make the bar plot\n",
        "p3 = axes[0, 1].bar(Pclass_survived.index, Pclass_survived.values)\n",
        "p4 = axes[0, 1].bar(Pclass_not_survived.index, Pclass_not_survived.values, bottom=Pclass_survived.values)\n",
        "axes[0, 1].set_title('Pclass', fontsize=25)\n",
        "axes[0, 1].set_ylabel('Counts', fontsize=20)\n",
        "axes[0, 1].tick_params(axis='both', labelsize=15)\n",
        "axes[0, 1].legend((p3[0], p4[0]), ('Survived', 'Not-survived'), fontsize = 15) #display legend categories\n",
        "\n",
        "# - - make the data read to feed into the visulizer\n",
        "Embarked_survived = data.replace({'Survived': {1: 'Survived', 0: 'Not-survived'}})[data['Survived']==1]['Embarked'].value_counts()\n",
        "Embarked_not_survived = data.replace({'Survived': {1: 'Survived', 0: 'Not-survived'}})[data['Survived']==0]['Embarked'].value_counts()\n",
        "Embarked_not_survived = Embarked_not_survived.reindex(index = Embarked_survived.index) # Embarked_survived as index at the bottom\n",
        "\n",
        "# Make the bar plot\n",
        "p5 = axes[1, 0].bar(Embarked_survived.index, Embarked_survived.values)\n",
        "p6 = axes[1, 0].bar(Embarked_not_survived.index, Embarked_not_survived.values, bottom=Embarked_survived.values)\n",
        "axes[1, 0].set_title('Embarked', fontsize=25)\n",
        "axes[1, 0].set_ylabel('Counts', fontsize=20)\n",
        "axes[1, 0].tick_params(axis='both', labelsize=15)\n",
        "axes[1, 0].legend((p5[0], p5[0]), ('Survived', 'Not-survived'), fontsize = 15) #display legend categories\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B3pn46TlDaB"
      },
      "source": [
        "**Feature selection and feature engineering**\n",
        "\n",
        "In this step, we will do lots of things to our data such as \n",
        "\n",
        "1) Drop more features if we think they are not helpful to our model from the analysis above.\n",
        "\n",
        "2) Deal with missing values.\n",
        "\n",
        "3) Deal with the highly skewed dataset.\n",
        "\n",
        "4) One Hot Encoding for the categorical features.\n",
        "\n",
        "@ @ @"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw8TQKSfu2B_"
      },
      "source": [
        "1) Drop features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzDaz9qiJQVv"
      },
      "source": [
        "# Drop features from the dataset\n",
        "\n",
        "#data = data.drop(['PassengerId', 'Ticket', 'Cabin', 'Name'])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ozk1LEClSUQ"
      },
      "source": [
        "2) Deal with missing values\n",
        "\n",
        "- Omission: If only a small number of records have missing values, can omit them. But it is not practical to do so if many records have missing values, or the cost of omitting the record is too high such as the medical study's dataset.\n",
        "\n",
        "- Imputation: Do your best guess. Can use average to replace missing data blank.\n",
        "\n",
        "  We will impute record for this project. And this fillin function can be apply for other dataset too !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQV857HGlQtF"
      },
      "source": [
        "# Filling in missing values for 'Age', 'Embarked'\n",
        "# Age - use 'median value' \n",
        "def fill_na(data, inplace=True):\n",
        "    return data.fillna(data.median(), inplace=inplace)\n",
        "\n",
        "# Use data['Age'] to replace data, will return data['Age'].median()\n",
        "fill_na(data['Age'])\n",
        "\n",
        "# Check the result\n",
        "data['Age'].describe()\n",
        "\n",
        "# Embarked - use 'S' since there are only 2 missing and S is the most represent.\n",
        "def fill_na_2(data, inplace=True):\n",
        "    return data.fillna('S', inplace=inplace)\n",
        "\n",
        "fill_na_2(data['Embarked'])\n",
        "\n",
        "# Check the result\n",
        "data['Embarked'].describe()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l79sBNeuvEmc"
      },
      "source": [
        "3) Deal with the highly skewed dataset\n",
        "\n",
        "- Can normalize the data when variables with the largest scales would dominate and skew results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0h-vYt3ldUZ"
      },
      "source": [
        "# Log-transformation of the fare\n",
        "# 'Fair' is highly right-skewed\n",
        "\n",
        "# import package\n",
        "import numpy as np\n",
        "\n",
        "# log-transformation\n",
        "def log_transformation(data):\n",
        "    return data.apply(np.log1p)\n",
        "\n",
        "data['Fare_log1p']=log_transformation(data['Fare'])\n",
        "\n",
        "# Chenck the result\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzgqj4NTleaL"
      },
      "source": [
        "# Use distribution to check fransformed 'Fare'\n",
        "\n",
        "# Set up the figure size\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "\n",
        "# Draw histograms\n",
        "plt.hist(data['Fare_log1p'], bins = 40)\n",
        "plt.xlabel('Fare_log1p', fontsize = 20)\n",
        "plt.ylabel('Counts', fontsize = 20)\n",
        "plt.tick_params(axis = 'both', labelsize=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrwP5vS4vITA"
      },
      "source": [
        "4) One Hot Encoding for the categorical features:\n",
        "\n",
        "- Convert categorical data to numerical data (create binary dummies or indicator variables)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHKmboCkusrH"
      },
      "source": [
        "# Get the categorical data\n",
        "\n",
        "cat_features = ['Pclass', 'Sex', 'Embarked']\n",
        "data_cat = data[cat_features]\n",
        "print(data_cat.head(3))\n",
        "\n",
        "# Pclass looks like numbers but actually a categorical variable, replace the data to words first.\n",
        "\n",
        "data_cat = data_cat.replace({'Pclass':{1:'1st', 2:'2nd', 3:'3rd'}})\n",
        "print(data_cat.head(3))\n",
        "\n",
        "# One hot encoding\n",
        "data_cat_dummies = pd.get_dummies(data_cat)\n",
        "data_cat_dummies.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjBWrHXi4c6X"
      },
      "source": [
        "**The introduction to the goal of this project and the methodology for preprocessing the data ends here. After the data preprocessing, we can move on to partitioning the data in order to build and assess the model.**\n",
        "\n",
        "The following:\n",
        "\n",
        "- Partitioning the dataset\n",
        "  * Problem: How well will our model perform with new data?\n",
        "  * Solution: Separate data into two parts: Training and Validation.(Or separate data into three parts: Training, Validation, and Test.)\n",
        "- Train the model\n",
        "- Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ALorGZr4R6D"
      },
      "source": [
        "# Create all feature dataset: Combine the numerical features and the dummies features (data_cat_dummies) together\n",
        "# num_features = ['Age', 'SibSp', 'Parch', 'Fare'] can't use, need \"transformed fare\"\n",
        "# Can do another test model using original fare to see the result.\n",
        "\n",
        "features_model = ['Age', 'SibSp', 'Parch', 'Fare_log1p']\n",
        "dataset_features_x = pd.concat([data[features_model], data_cat_dummies], axis=1)\n",
        "\n",
        "# Create a whole target dataset\n",
        "# Our target variable is 'Survived', but it actually contain 0 & 1\n",
        "\n",
        "dataset_targets_y = data.replace({'Survived':{1:'Survived', 0: 'Not_survived'}})['Survived']\n",
        "\n",
        "# Dataset is ready above\n",
        "# Partitioning dataset into training and validation\n",
        "\n",
        "# Import packages\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Partitioning the dataset\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=?, random_state=?)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(dataset_features_x, dataset_targets_y, test_size = 0.3, random_state=11)\n",
        "\n",
        "# Number of samples in each dataset - Make sure the shape of x,y dataset matches\n",
        "\n",
        "print(\"For features-No. of samples in training set: \", x_train.shape)\n",
        "print(\"For features-No. of samples in test set: \", x_val.shape)\n",
        "print(\"For targets-No. of samples in training set:\", y_train.shape)\n",
        "print(\"For targets-No. of samples in test set: \", y_val.shape)\n",
        "\n",
        "# Details of target dataset (Survived and not-survived details)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"No. of survived and not-survived in the training set: \")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"No. of survived and not-survived in the validation set: \")\n",
        "print(y_val.value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQgxR12SaN-2"
      },
      "source": [
        "To predict a passenger survived or not - a classification problem.\n",
        "- We are using two models **Logistic regressoin** and **Decision tree**\n",
        "\n",
        "To evaluate a binary prediction performance (classification)\n",
        "\n",
        "- We are using **Confusion Matrix, precision, recall, F1 score,** and **ROC Curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY0AysEUPuAr"
      },
      "source": [
        "# LogisticRegression\n",
        "# Import packages\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from yellowbrick.classifier import ClassificationReport\n",
        "from yellowbrick.classifier import ROCAUC\n",
        "\n",
        "# Instantiate the classification model\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "# The ConfusionMatrix Visualizer taxes a model\n",
        "\n",
        "cm = ConfusionMatrix(model, classes=['Not_survived', 'Survived'])\n",
        "\n",
        "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
        "\n",
        "cm.fit(x_train, y_train)\n",
        "\n",
        "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
        "# and then creates the confusion_matrix from scikit-learn.\n",
        "\n",
        "cm.score(x_val, y_val)\n",
        "\n",
        "# change fontsize of the labels in the figure\n",
        "\n",
        "for label in cm.ax.texts:\n",
        "    label.set_size(25)\n",
        "\n",
        "# How did we do?\n",
        "cm.poof()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG75jHf9kkPg"
      },
      "source": [
        "# LogisticRegression\n",
        "\n",
        "|| Confusion Matrix can tell us the accuracy of our prediction ||\n",
        "\n",
        "**True Positive**: If the Actual Outcome is 1 (Survived), and the Predicted result is also 1.\n",
        "\n",
        "**True Negative**: If the Actual Outcome is 0 (Not_Survived), and the Predicted result is also 0.\n",
        "\n",
        "**False Positive**: If the Actual Outcome is 0, but the Predicted result is also 1.\n",
        "\n",
        "**False Negative**: If the Actual Outcome is 1, but the Predicted result is also 0.\n",
        "\n",
        "||Total Accuracy||\n",
        "\n",
        "To get the Accuray of the model, we need to know what is the percentage of \"**the number of correct predictions (True Positive and True Negative)**\" over \"**the number of all the prediction**\".\n",
        "\n",
        "Number of correct predictions: 158+68=226\n",
        "\n",
        "Total prediction: 158+18+68+24=268\n",
        "\n",
        "Total Accuracy: 226/268= 84.33% \n",
        "\n",
        "The accuracy is not that bad.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2mt9D6zutb5"
      },
      "source": [
        "# Logistic Regression\n",
        "# Precision, Recall, and F1 Score\n",
        "# Set the size of the figure and the font size\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15, 7)\n",
        "plt.rcParams['font.size'] = 20\n",
        "\n",
        "# Instantiate the visualizer\n",
        "\n",
        "visualizer = ClassificationReport(model, classes=['Not_survived', 'Survived'])\n",
        "visualizer.fit(x_train, y_train)          # Fit the training dataset to the visualizer\n",
        "visualizer.score(x_val, y_val)            # Evaluate the model on the validation dataset\n",
        "visualizer.poof()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIHuEjjWuCOM"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "|| Precision, Recall, and F1 Score ||\n",
        "\n",
        "Also show the performance is pretty good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gXC0bRvtDRm"
      },
      "source": [
        "# Logistic Regression\n",
        "# ROC Curve and AUC\n",
        "\n",
        "# Instantiate the visualizer\n",
        "visualizer = ROCAUC(model)\n",
        "\n",
        "visualizer.fit(x_train, y_train)   # Fit the training dataset to the visualizer\n",
        "visualizer.score(x_val, y_val)     # Evaluate the model on the validation dataset\n",
        "visualizer.poof()                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frm_24t9undE"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "**ROC Curve**:\n",
        "\n",
        "The dotted diagonal line corresponds to random ranking. The ROC Curve of a stong model will be far above that line.\n",
        "\n",
        "**Area Under the Curve(AUC)**:\n",
        "- Measures model performance from 0.5 to 1.0\n",
        "- AUC < 0.6 is a weak model\n",
        "- AUC > 0.7 is a strong model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fOPa6JJz_iz"
      },
      "source": [
        "# Decision Tree\n",
        "# Import packages\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Instantiate the classification model\n",
        "\n",
        "model_tree = DecisionTreeClassifier()\n",
        "\n",
        "# The ConfusionMatrix Visualizer taxes a model\n",
        "\n",
        "cm = ConfusionMatrix(model_tree, classes=['Not_survived', 'Survived'])\n",
        "\n",
        "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
        "\n",
        "cm.fit(x_train, y_train)\n",
        "\n",
        "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
        "# and then creates the confusion_matrix from scikit-learn.\n",
        "\n",
        "cm.score(x_val, y_val)\n",
        "\n",
        "# change fontsize of the labels in the figure\n",
        "\n",
        "for label in cm.ax.texts:\n",
        "    label.set_size(25)\n",
        "\n",
        "# How did we do?\n",
        "cm.poof()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zr0rDQ0wiB"
      },
      "source": [
        "# Decision Tree\n",
        "\n",
        "||Total Accuracy||\n",
        "\n",
        "Number of correct predictions: 148+65=213\n",
        "\n",
        "Total prediction: 148+28+65+27=268\n",
        "\n",
        "Total Accuracy: 226/268= 79.48% \n",
        "\n",
        "The accuracy is lower than the Logistic regression's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-fS2DdT2MAv"
      },
      "source": [
        "# Decision Tree\n",
        "# ROC Curve and AUC\n",
        "\n",
        "# Instantiate the visualizer\n",
        "visualizer = ROCAUC(model_tree)\n",
        "\n",
        "visualizer.fit(x_train, y_train)   # Fit the training dataset to the visualizer\n",
        "visualizer.score(x_val, y_val)     # Evaluate the model on the validation dataset\n",
        "visualizer.poof() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZSbqOOf2-3y"
      },
      "source": [
        "# Decision Tree\n",
        "\n",
        "**ROC Curve and AUC**:\n",
        "\n",
        "Compare to Logistic regression's ROC Curve, the Decision tree's curve is clearly weaker, closer to the dotted diagonal line.\n",
        "\n",
        "If I have to choose between the two models, I'll use the Logistic regression model for this project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7xPdGoynyyf"
      },
      "source": [
        "# Thanks\n",
        "\n",
        "This is my first project and I refer to some resources and course materials, I appreciate them helping me grow.\n",
        "\n",
        "Reference:\n",
        "\n",
        "*How to Start Your First Data Science Project*\n",
        "https://www.districtdatalabs.com/how-to-start-your-first-data-science-project By Juan L. Kehoe\n",
        "\n",
        "*UConn Predictive Modeling course* by Jennifer Eigo\n",
        "\n",
        "*scikit-learn* https://scikit-learn.org/stable/index.html\n",
        "\n",
        "And many other discussion boards."
      ]
    }
  ]
}